{"paragraphs":[{"text":"val numPartitions = 8\n\nval input = sc.textFile(\"hdfs://localhost:9000/collector_output.txt\", numPartitions)\n// val input = sc.textFile(\"hdfs://localhost:9000/input.txt\", numPartitions)//.sample(false, 0.01)\n\n// maps string input user;clip;rating to tuple of ints (user, clip, rating)\nval userClipRating = input.map(line => line.split(';').map(s => s.toInt)).map(arr => (arr(0), arr(1), arr(2)))\n\n// sorted (clip, rating)\nval sortedClipRating = userClipRating.map(t => (t._2, t._3)).sortBy(identity)","user":"anonymous","dateUpdated":"2019-06-02T19:36:20+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"numPartitions: Int = 8\ninput: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/input.txt MapPartitionsRDD[392] at textFile at <console>:31\nuserClipRating: org.apache.spark.rdd.RDD[(Int, Int, Int)] = MapPartitionsRDD[394] at map at <console>:34\nsortedClipRating: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[400] at sortBy at <console>:37\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=57"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559398177201_-1740440410","id":"20190601-160937_343840835","dateCreated":"2019-06-01T16:09:37+0200","dateStarted":"2019-06-02T09:45:28+0200","dateFinished":"2019-06-02T09:45:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:759"},{"text":"input.count()\nuserClipRating.count()\nsortedClipRating.count()\nsortedClipRating.take(110)","user":"anonymous","dateUpdated":"2019-06-02T10:03:35+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res122: Array[(Int, Int)] = Array((1,7), (1,8), (1,8), (1,8), (1,9), (1,9), (1,10), (1,10), (1,10), (1,10), (1,10), (2,4), (2,5), (2,6), (2,6), (2,7), (2,7), (2,9), (3,6), (3,7), (3,8), (3,8), (3,10), (3,10), (4,4), (4,5), (4,6), (4,6), (4,6), (4,6), (4,8), (4,8), (4,9), (4,9), (4,10), (5,3), (5,4), (5,5), (5,6), (5,6), (5,6), (5,7), (5,7), (5,7), (5,8), (5,8), (5,8), (5,10), (5,10), (6,1), (6,2), (6,2), (6,2), (6,5), (6,5), (7,2), (7,2), (7,4), (7,6), (7,6), (7,6), (7,6), (7,8), (7,8), (7,8), (7,8), (7,9), (7,10), (7,10), (7,10), (8,2), (8,3), (8,5), (8,6), (8,6), (8,7), (8,8), (8,8), (8,9), (8,10), (9,2), (9,4), (9,6), (9,6), (9,6), (9,7), (9,7), (9,8), (9,9), (9,10), (10,5), (10,7), (10,9), (10,9), (10,10), (10,10), (10,10), (10,10), (10,10), (10,10), (10,10), (10,10), (11,6), (11,6)..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=179","http://192.168.1.228:4040/jobs/job?id=180","http://192.168.1.228:4040/jobs/job?id=181","http://192.168.1.228:4040/jobs/job?id=182"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559461538261_1053969643","id":"20190602-094538_1000574106","dateCreated":"2019-06-02T09:45:38+0200","dateStarted":"2019-06-02T10:03:35+0200","dateFinished":"2019-06-02T10:03:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:760"},{"text":"// sums all counts and returns them as single element iterator\ndef sumIterator(clipCount: Iterator[(Int, Int)]): Iterator[Int] = Seq(clipCount.foldLeft(0: Int){ (acc, clipCounts) => acc + clipCounts._2 }).iterator\n\n// scanLeft (clip, count, sumPrefix, sumPrefix + count) -> drop first -> (clip, count, sumPrefix)\ndef prefixSumIterator(sortedCountsPartition: Iterator[(Int, Int)], sumPrefix: Iterator[Int]): Iterator[(Int, Int, Int)] = sortedCountsPartition.scanLeft((0, 0, 0, sumPrefix.next)){ (acc, clipCount) => (clipCount._1, clipCount._2, acc._4, acc._4 + clipCount._2) }.drop(1).map(t => (t._1, t._2, t._3))\n\n// compute global indices of median for clip\ndef computeMedianIndices(clip: Int, count: Int, sumPrefix: Int): Seq[(Int, Int)] = if (count % 2 == 1) Seq((clip, sumPrefix + count / 2)) else Seq((clip, sumPrefix + count / 2 - 1), (clip, sumPrefix + count / 2))\n\n// counts number of ratings per clip, sorted (clip, count)\nval sortedCounts = sortedClipRating.map(t => (t._1, 1)).reduceByKey(_ + _).sortByKey()\n\n// sum counts on every partition, single countSum per partition\nval countRatingsPerPartition = sortedCounts.mapPartitions(sumIterator)\n\n// collects countSums to master node, count prefixSum, drop last element, distribute, prefixSums\nval prefixSumRatingsPerPartition = sc.parallelize(countRatingsPerPartition.collect.scan(0)(_ + _).take(numPartitions), numPartitions)\n\n// zip sorted (clip, count) with prefixSums, compute prefixSum for every element globally, (clip, count, sumPrefix)\nval clipCountPrefixSum = sortedCounts.zipPartitions(prefixSumRatingsPerPartition)(prefixSumIterator)\n\n// (clip, medianIndex)\nval medianIndices = clipCountPrefixSum.flatMap(t => computeMedianIndices(t._1, t._2, t._3))","user":"anonymous","dateUpdated":"2019-06-02T09:45:28+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sumIterator: (clipCount: Iterator[(Int, Int)])Iterator[Int]\nprefixSumIterator: (sortedCountsPartition: Iterator[(Int, Int)], sumPrefix: Iterator[Int])Iterator[(Int, Int, Int)]\ncomputeMedianIndices: (clip: Int, count: Int, sumPrefix: Int)Seq[(Int, Int)]\nsortedCounts: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[405] at sortByKey at <console>:47\ncountRatingsPerPartition: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[406] at mapPartitions at <console>:50\nprefixSumRatingsPerPartition: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[407] at parallelize at <console>:53\nclipCountPrefixSum: org.apache.spark.rdd.RDD[(Int, Int, Int)] = ZippedPartitionsRDD2[408] at zipPartitions at <console>:56\nmedianIndices: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[409] at flatMap at..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=58","http://192.168.1.228:4040/jobs/job?id=59"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559398243973_-403281235","id":"20190601-161043_397942293","dateCreated":"2019-06-01T16:10:43+0200","dateStarted":"2019-06-02T09:45:28+0200","dateFinished":"2019-06-02T09:45:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:761"},{"text":"sortedCounts.count()\nsortedCounts.take(80)\ncountRatingsPerPartition.count\ncountRatingsPerPartition.collect\n// print(countRatingsPerPartition.collect.mkString(\", \") + \"\\n\")\nprefixSumRatingsPerPartition.collect\nclipCountPrefixSum.count\nclipCountPrefixSum.filter(t => t._1 > 200).collect\nmedianIndices.count\nmedianIndices.collect","user":"anonymous","dateUpdated":"2019-06-02T10:13:16+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res137: Array[(Int, Int)] = Array((1,5), (2,14), (3,20), (3,21), (4,29), (5,41), (5,42), (6,51), (6,52), (7,62), (8,74), (8,75), (9,84), (9,85), (10,95), (10,96), (11,105), (12,112), (12,113), (13,121), (14,130), (15,140), (16,151), (17,162), (18,172), (18,173), (19,182), (19,183), (20,193), (21,201), (21,202), (22,209), (22,210), (23,218), (23,219), (24,225), (24,226), (25,231), (25,232), (26,237), (26,238), (27,244), (28,251), (28,252), (29,259), (29,260), (30,267), (31,277), (32,288), (33,297), (33,298), (34,307), (34,308), (35,315), (35,316), (36,325), (36,326), (37,340), (38,352), (39,361), (40,372), (40,373), (41,384), (41,385), (42,392), (42,393), (43,396), (43,397), (44,401), (45,411), (46,421), (46,422), (47,432), (48,441), (49,449), (50,461), (50,462), (51,472), (52,482), (53,..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=254","http://192.168.1.228:4040/jobs/job?id=255","http://192.168.1.228:4040/jobs/job?id=256","http://192.168.1.228:4040/jobs/job?id=257","http://192.168.1.228:4040/jobs/job?id=258","http://192.168.1.228:4040/jobs/job?id=259","http://192.168.1.228:4040/jobs/job?id=260","http://192.168.1.228:4040/jobs/job?id=261","http://192.168.1.228:4040/jobs/job?id=262"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559461686606_1115048745","id":"20190602-094806_473021294","dateCreated":"2019-06-02T09:48:06+0200","dateStarted":"2019-06-02T10:13:16+0200","dateFinished":"2019-06-02T10:13:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:762"},{"text":"// given iterable, compute mean\ndef computeMeanFromIterable(elems: Iterable[Int]): Float = { val arr = elems.toArray; arr.sum.toFloat / arr.length }\n\n// selects only these (clip, rating), which are median\n// sorted (clip, rating) -> ((clip, rating), index) -> (clip, (rating, index)) + (clip, medianIndex) -> (clip, ((rating, index), medianIndex)) -> unsorted (clip, rating) with duplicates on even counts\n// val filteredClipRating = sortedClipRating.zipWithIndex.map(t => (t._1._1, (t._1._2, t._2))).join(medianIndices).filter(t => t._2._1._2 == t._2._2).map(t => (t._1, t._2._1._1))\n\n// sorted (clip, medianIndex) -> (medianIndex, clip)\n// sorted (clip, rating) -> ((clip, rating), index) -> (index, rating)\n// + -> (index, (clip, rating))\nval filteredClipRating = medianIndices.map(_.swap).join(sortedClipRating.zipWithIndex.map(t => (t._2.toInt, t._1._2))).map(_._2)\n\n// (clip, mean)\nval clipMean = filteredClipRating.groupByKey.map(t => (t._1, computeMeanFromIterable(t._2)))","user":"anonymous","dateUpdated":"2019-06-02T10:38:42+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"computeMeanFromIterable: (elems: Iterable[Int])Float\nfilteredClipRating: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[449] at map at <console>:52\nclipMean: org.apache.spark.rdd.RDD[(Int, Float)] = MapPartitionsRDD[451] at map at <console>:55\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=267"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559398301495_362143379","id":"20190601-161141_1121335567","dateCreated":"2019-06-01T16:11:41+0200","dateStarted":"2019-06-02T10:37:29+0200","dateFinished":"2019-06-02T10:37:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:763"},{"text":"filteredClipRatings.count\nfilteredClipRatings.collect\nclipMean.count\nclipMean.collect","user":"anonymous","dateUpdated":"2019-06-02T10:39:48+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res143: Array[(Int, Float)] = Array((656,1.5), (440,5.0), (600,5.5), (320,4.0), (752,7.0), (856,5.0), (744,1.0), (408,9.5), (80,1.0), (672,4.5), (904,1.5), (464,7.0), (952,2.0), (480,3.0), (352,5.5), (104,5.5), (24,4.5), (520,8.0), (272,7.0), (152,4.0), (912,3.0), (368,3.5), (624,4.0), (552,4.0), (288,9.0), (400,1.5), (576,2.5), (864,4.5), (888,6.0), (976,5.0), (816,8.0), (256,2.0), (712,6.0), (728,5.0), (192,1.0), (224,4.0), (160,8.5), (512,7.0), (112,8.0), (568,1.0), (120,3.0), (504,4.0), (992,2.0), (424,10.0), (184,3.0), (632,8.5), (200,2.0), (616,6.5), (544,2.0), (584,3.0), (808,7.0), (608,10.0), (696,6.0), (528,7.0), (264,1.0), (40,6.5), (832,9.0), (304,5.5), (872,8.0), (496,10.0), (944,6.5), (136,2.0), (1000,7.0), (176,9.0), (328,2.0), (360,8.0), (840,2.0), (168,10.0), (312,6.5), ..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=274","http://192.168.1.228:4040/jobs/job?id=275","http://192.168.1.228:4040/jobs/job?id=276","http://192.168.1.228:4040/jobs/job?id=277"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559463332226_1146696475","id":"20190602-101532_297998356","dateCreated":"2019-06-02T10:15:32+0200","dateStarted":"2019-06-02T10:39:48+0200","dateFinished":"2019-06-02T10:39:48+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:764"},{"text":"// (user, clip, rating) -> (clip, (user, rating)) + (clip, mean) -> (clip, ((user, rating), mean)) -> user\nval badUsers = userClipRating.filter(t => t._3 == 1 || t._3 == 10).map(t => (t._2, (t._1, t._3))).join(clipMean).filter(t => math.abs(t._2._1._2.toFloat - t._2._2) >= 5).map(t => t._2._1._1).sortBy(identity)\nbadUsers.collect()","user":"anonymous","dateUpdated":"2019-06-02T11:02:22+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"badUsers: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[487] at sortBy at <console>:46\nres146: Array[Int] = Array(355, 567, 972, 1199, 1205, 1673, 2342, 2397, 2643, 3097, 3156, 3247, 3406, 3617, 3831, 4320, 4388, 4485, 4783, 4792, 5069, 5100, 5219, 5437, 5475, 5715, 5925, 6755, 6968, 7004, 7146, 7223, 7351, 7467, 7563, 8172, 8624, 8738, 8755, 9115, 9569)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.228:4040/jobs/job?id=282","http://192.168.1.228:4040/jobs/job?id=283"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1559415126421_1083292783","id":"20190601-205206_224709810","dateCreated":"2019-06-01T20:52:06+0200","dateStarted":"2019-06-02T11:02:22+0200","dateFinished":"2019-06-02T11:02:22+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:765"},{"user":"anonymous","dateUpdated":"2019-06-02T09:45:30+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559431012434_295432906","id":"20190602-011652_272162501","dateCreated":"2019-06-02T01:16:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:766"}],"name":"Analyser","id":"2EBQ64TT5","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}